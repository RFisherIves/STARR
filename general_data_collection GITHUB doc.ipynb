{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STARR Project - General Data Collection Routine - Version 2.0\n",
    "Author: George Gorospe\n",
    "\n",
    "Version Notes:\n",
    "2.0 - Added new functions to control robot motion while collecting image data.\n",
    "\n",
    "Note: This notebook draws heavily from Nvidia's jetbot code base found at: https://github.com/NVIDIA-AI-IOT/jetbot\n",
    "\n",
    "Before we can get started we need to ensure that we have the jetbot module installed. The jetbot module provides some useful tools that will help us collect data.\n",
    "\n",
    "If you encounter an error early in the is notebook it is likely that you'll need to downdload and install the jetbot module. To do this follow the next two steps:\n",
    "1. From a terminal on the nano, use: git clone https://github.com/NVIDIA-AI-IOT/jetbot.git\n",
    "2. Once inside your new jetbot folder, use: sudo python3 setup.py install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "We can use this basic routine for collecting and labeling different types of data.\n",
    "Start by entering the name of the object you're photographing. The routine will create a new folder in your data sets directory for the class of data you're collecting.\n",
    "\n",
    "You'll want to take multiple photos of the object all by itself in differient orientations, locations, and lighting conditions. A good combination of all of these will ensure a robust AI capable of identifying the object with high accuracy. In general, you'll want to collect at least 100 images.\n",
    "\n",
    "If you are collecting location data, i.e. gym, living room, kitchen, classroom, make sure to take photos of all the objects that will always be in the room. This means taking photos of the walls, desks, tables, trashcans, but not backpacks, coats, or note books.\n",
    "\n",
    "Final Note: This is experimental software, it has been tested in limited environments and may not do everything we expect. Use it, experiment with it, and feel free to change it. If you break the software you can always download it again from the source and experiment more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display live camera feed\n",
    "\n",
    "To start, we'll initialize and display the feed from our camera.\n",
    "Important part here is that we size the image to fit what our neural network.\n",
    "For certain tasks it may be better to collect larger images then downscale later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98d42e286f645a2929a05e79956bdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First we'll import some tools to help us get the camera started and to make this note book interactive\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import Image, display\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to label the data we'll be collecting:\n",
    "use the text box to create a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f623799fe4089b88a230db2fcad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='structured_string', description='Enter Label:', placeholder='structured_stri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os  # This library allows us to explore and modify the file structure on the Nano\n",
    "# Creating a function to add a folder to our directory\n",
    "def folder_function(label):\n",
    "    # Using the \"try/except\" statement here because the makedirs function can throw an error if the directory exists already\n",
    "    try:\n",
    "        os.makedirs('data/'+label)\n",
    "        print(\"Creating data folder: data/\"+label)            \n",
    "    except:\n",
    "        print('Directory no created because it already exists')\n",
    "    \n",
    "# Creating the interactive text box widget\n",
    "# Creating the interactive text box widget\n",
    "interactiveTextBox = interactive(folder_function,{'manual': True}, label = widgets.Text(value='structured_string',placeholder = 'structured_string',description='Enter Label:'));\n",
    "interactiveTextBox # calling the interactive element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you refresh the Jupyter file browser on the left, you should now see a new directory with your label.  \n",
    "\n",
    "Now we have a folder with our label, we're nearly ready to start saving images inside our folder.\n",
    "It is important to understand that the name of the image file is not important for our training purposes, only the folder name is important.\n",
    "This means that the images can have names like img1.jpg, tm223.jpg, cat.jpg, ect. But if they are all in a folder titled, \"dog\" then they will be interpreted as dogs.\n",
    "\n",
    "Since we don't want to manually name each image we collect, we'll use the ``uuid`` package in python, which defines the ``uuid1`` method to generate\n",
    "a unique identifier that we can use as the file name.  This unique identifier is generated from information like the current time and the machine address, and helps us make sure that we will never overwrite data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "# the save snapshot function will collect an image and save it to file\n",
    "# This is a callback function, it is executed when we press a button to collect images\n",
    "def save_snapshot(directory):\n",
    "    global image_count\n",
    "    image_path = os.path.join(directory, str(uuid1()) + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image.value)\n",
    "        image_count.value = len(os.listdir(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to create the functions we'll use to control the robot motion. To do this we need the robot class from the jetbot library, it contains the basic motor control functions associated with our hardware. Additionally, we'll import the time library so that we can control our robot's motion for a specific duration.\n",
    "\n",
    "There are 4 key functions based on the motions we want for the robot, each involves setting motor speed and direction for a specif duration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "import time\n",
    "\n",
    "# Robot object\n",
    "robot = Robot()\n",
    "\n",
    "# Our robot motion control functions, they use speed, direction, and duration.\n",
    "def robotLeft():\n",
    "    robot.left(speed=0.6)\n",
    "    time.sleep(0.3)\n",
    "    robot.stop()\n",
    "\n",
    "def robotRight():\n",
    "    robot.right(speed=0.6)\n",
    "    time.sleep(0.5)\n",
    "    robot.stop()\n",
    "    \n",
    "def robotForward():\n",
    "    robot.forward(speed=0.6)\n",
    "    time.sleep(0.5)\n",
    "    robot.stop()\n",
    "\n",
    "def robotBackward():\n",
    "    robot.backward(speed=0.6)\n",
    "    time.sleep(0.5)\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the functions for collecting images and controlling the motion of the robot, we can start to put together the graphical user interface we'll use for actually commanding the robot.\n",
    "\n",
    "Within Jupyter Notebooks we'use using ipython widgets to do this. Specifically we're using lots of buttons, and a layout function.\n",
    "\n",
    "We'll start by constructing the user interface elements, then at the end we'll display the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "/home/jetbot/Documents/Prototype\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dc86739c8043078ceed17909cef191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Button(button_style='success', description='LEFT', layout=Layout(height='64px', width='128px'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98d42e286f645a2929a05e79956bdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0b2f7321894404a9ed71fcf0612085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=0, layout=Layout(height='64px', width='128px')), Button(button_style='success', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constructing the interative textbox for the number of photos and the button to collect images\n",
    "\n",
    "label = interactiveTextBox.children[0].value # take the label text from the interactive text box module\n",
    "directory = 'data/' + label\n",
    "print(os.listdir(directory))\n",
    "print(os.getcwd())\n",
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "collect_button = widgets.Button(description='Collect Image', button_style='success', layout=button_layout)\n",
    "image_count = widgets.IntText(layout=button_layout, value=len(os.listdir(directory)))\n",
    "\n",
    "# Constructing the robot motion control buttons and layout\n",
    "leftButton = widgets.Button(description='LEFT', button_style='success', layout=button_layout)\n",
    "rightButton = widgets.Button(description='RIGHT', button_style='success', layout=button_layout)\n",
    "forwardButton = widgets.Button(description='FORWARD', button_style='success', layout=button_layout)\n",
    "backwardButton = widgets.Button(description='Backward', button_style='success',\n",
    "layout=button_layout)\n",
    "\n",
    "# Attach callbacks, references to the functions we created earlier, and arrange robot control widgets in a nice grid\n",
    "collect_button.on_click(lambda x: save_snapshot(directory))\n",
    "leftButton.on_click(lambda x: robotLeft())\n",
    "rightButton.on_click(lambda x: robotRight())\n",
    "forwardButton.on_click(lambda x: robotForward())\n",
    "backwardButton.on_click(lambda x: robotBackward())\n",
    "items = [leftButton, forwardButton, backwardButton, rightButton]\n",
    "\n",
    "display(widgets.Box(items))\n",
    "display(image)\n",
    "display(widgets.HBox([image_count, collect_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a nice interface for controlling your robot and collecting data. You can now go out and start collecting high quality images with your robot.\n",
    "\n",
    "Collect images of things that are the same class. Cups, shoes, books, backpacks.\n",
    "Vary the position and orientation of the object, the lighting, the ground surface.\n",
    "Try to have limited other things in the background, walls are fine, but you don't want to have other class objects.\n",
    "This means no banana for scale. Just the object we're interested in.\n",
    "\n",
    "Here are some tips for labeling data\n",
    "\n",
    "1. Try different orientations\n",
    "2. Try different lighting\n",
    "3. Try varied object / collision types; walls, ledges, objects\n",
    "4. Try different textured floors / objects;  patterned, smooth, glass, etc.\n",
    "\n",
    "Ultimately, the more data we have of scenarios the robot will encounter in the real world, the better our object classification and navigation (collision avoidance) performance  will be.  It's important\n",
    "to get *varied* data (as described by the above tips) and not just a lot of data, but you'll probably need at least 100 images of each class (that's not a science, just a helpful tip here).  But don't worry, it goes pretty fast once you get going :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "Once you've collected lots of images of each class, go back up to the top of the notebook and start again with a new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
